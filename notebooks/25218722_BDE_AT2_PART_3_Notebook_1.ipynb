{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8800620237287761,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7711dec-145c-4f17-9bf7-06c9f2c93b00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PART-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8800620237287761,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f711bc-9d23-4c3b-ad20-6bf30fba5462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----\n",
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b39150-3cfd-4dbc-8723-e88373233d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import PySpark SQL functions module\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8da961-6d38-4102-abc8-85cd45c206e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load full taxi data with time features\n",
    "df = spark.table(\"final_taxi_data\") \\\n",
    "    .withColumn(\"year\", F.year(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"month\", F.month(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"hour\", F.hour(\"pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4327acd0-e391-45f1-bed6-262bd1f523bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "train_df = df.filter(\n",
    "    ((F.col(\"year\") >= 2009) & (F.col(\"year\") <= 2023)) |\n",
    "    ((F.col(\"year\") == 2024) & (F.col(\"month\") <= 9))\n",
    ")\n",
    "\n",
    "test_df = df.filter(\n",
    "    (F.col(\"year\") == 2024) & (F.col(\"month\") >= 10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f731ff9-a52e-4769-8347-0bebf7bcab57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Baseline model (train only)\n",
    "baseline_stats = train_df.groupBy(\n",
    "    \"taxi_colour\",\n",
    "    \"pickup_borough\",\n",
    "    \"dropoff_borough\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"hour\"\n",
    ").agg(F.avg(\"total_amount\").alias(\"baseline_pred\"))\n",
    "\n",
    "# Join predictions with train set\n",
    "train_with_pred = train_df.join(\n",
    "    baseline_stats,\n",
    "    on=[\"taxi_colour\", \"pickup_borough\", \"dropoff_borough\", \"month\", \"day_of_week\", \"hour\"],\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdef055-eb55-4f17-98fc-352ab1aac0c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE on train set: 8.26\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation on train\n",
    "rmse_df = train_with_pred.withColumn(\n",
    "    \"squared_error\",\n",
    "    (F.col(\"total_amount\") - F.col(\"baseline_pred\"))**2\n",
    ")\n",
    "\n",
    "baseline_rmse = rmse_df.agg(\n",
    "    F.sqrt(F.avg(\"squared_error\")).alias(\"rmse\")\n",
    ").collect()[0][\"rmse\"]\n",
    "\n",
    "print(f\"Baseline RMSE on train set: {baseline_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46833dae-2072-48f5-b5f9-ade9fe5f4b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Baseline model (test only)\n",
    "baseline_stats = test_df.groupBy(\n",
    "    \"taxi_colour\",\n",
    "    \"pickup_borough\",\n",
    "    \"dropoff_borough\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"hour\"\n",
    ").agg(F.avg(\"total_amount\").alias(\"baseline_pred\"))\n",
    "\n",
    "# Join predictions with train set\n",
    "test_with_pred = test_df.join(\n",
    "    baseline_stats,\n",
    "    on=[\"taxi_colour\", \"pickup_borough\", \"dropoff_borough\", \"month\", \"day_of_week\", \"hour\"],\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a535bda-d464-4638-80fd-63ac092d3193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE on test set: 10.21\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation on test\n",
    "rmse_df = test_with_pred.withColumn(\n",
    "    \"squared_error\",\n",
    "    (F.col(\"total_amount\") - F.col(\"baseline_pred\"))**2\n",
    ")\n",
    "\n",
    "baseline_rmse = rmse_df.agg(\n",
    "    F.sqrt(F.avg(\"squared_error\")).alias(\"rmse\")\n",
    ").collect()[0][\"rmse\"]\n",
    "\n",
    "print(f\"Baseline RMSE on test set: {baseline_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff46085-4468-455c-b581-0618d4841b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "------\n",
    "## Model - 1 (SGDRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066d7cb0-5735-4006-b90a-10505b8e1244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with only 2024 year data\n",
    "df_2024 = df.filter(F.col(\"year\") == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f018e42-2fe4-461e-aaa7-8e560c6172c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- VendorID: long (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- ehail_fee: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- trip_type: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- trip_time: decimal(27,6) (nullable = true)\n |-- speed_mph: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- taxi_colour: string (nullable = true)\n |-- pickup_borough: string (nullable = true)\n |-- pickup_zone: string (nullable = true)\n |-- dropoff_borough: string (nullable = true)\n |-- dropoff_zone: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- hour: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "df_2024.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64406cdc-842f-46e2-8e4c-281551aed5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns selected for modeling\n",
    "features = [\n",
    "    \"total_amount\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_time\",\n",
    "    \"speed_mph\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"hour\",\n",
    "    \"pickup_borough\",\n",
    "    \"dropoff_borough\",\n",
    "    \"RatecodeID\",\n",
    "    \"payment_type\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"ehail_fee\",\n",
    "    \"improvement_surcharge\",\n",
    "    \"congestion_surcharge\",\n",
    "    \"airport_fee\",\n",
    "    \"trip_type\",\n",
    "    \"taxi_colour\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7fbe02-be25-451d-b808-190e81646f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select only the required features\n",
    "df_2024 = df_2024.select(*features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc01517-0f91-471a-97bf-004e16383a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- total_amount: double (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- trip_time: decimal(27,6) (nullable = true)\n |-- speed_mph: double (nullable = true)\n |-- month: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- hour: integer (nullable = true)\n |-- pickup_borough: string (nullable = true)\n |-- dropoff_borough: string (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- payment_type: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- ehail_fee: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- trip_type: double (nullable = true)\n |-- taxi_colour: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Check the schema again\n",
    "df_2024.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ab10c9-6551-47df-9f9c-9cb5796bd7c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns for scikit-learn\n",
    "categorical_cols = [\"pickup_borough\", \"dropoff_borough\", \"RatecodeID\", \"payment_type\", \"trip_type\", \"taxi_colour\"]\n",
    "numerical_cols = [\n",
    "    \"passenger_count\", \"trip_distance\", \"trip_time\", \"speed_mph\", \"extra\", \"mta_tax\",\n",
    "    \"tip_amount\", \"ehail_fee\", \"improvement_surcharge\", \"congestion_surcharge\", \"airport_fee\",\n",
    "]\n",
    "# Time features are numerical, so add them\n",
    "numerical_cols.extend(['month', 'day_of_week', 'hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5bfd94-b6ee-4046-a7c8-5cd7c5dc2070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataframe into train and test dataframes\n",
    "train_df_2024= df_2024.filter((F.month(\"pickup_datetime\") <= 9))\n",
    "test_df_2024 = df_2024.filter((F.month(\"pickup_datetime\") >= 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c674f6-9651-4c57-89fa-b82115846c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the dataframes as Delta tables\n",
    "train_df_2024.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"train_df_2024_delta\")\n",
    "test_df_2024.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"test_df_2024_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540048ce-fe1c-4917-bf95-194c71242487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df_2024: rows=25689161, columns=21\ntest_df_2024: rows=9518622, columns=21\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the dataframes\n",
    "print(f\"train_df_2024: rows={train_df_2024.count()}, columns={len(train_df_2024.columns)}\")\n",
    "print(f\"test_df_2024: rows={test_df_2024.count()}, columns={len(test_df_2024.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec910e82-7c4c-4b57-a7fa-0f6be858a880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the PySpark DataFrame as a temporary view\n",
    "train_df_2024.createOrReplaceTempView(\"train_df_2024\")\n",
    "test_df_2024.createOrReplaceTempView(\"test_df_2024\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b660ce91-d324-4f78-aa70-55dbd4fdade9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the required Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline as sk_pipeline\n",
    "import joblib\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621591af-4cff-40d9-a9fd-520ecab6f3e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up dataset and columns\n",
    "df = train_df_2024 \n",
    "\n",
    "# Categorical and numerical columns\n",
    "categorical_cols = [\"pickup_borough\", \"dropoff_borough\", \"RatecodeID\", \"payment_type\", \"trip_type\", \"taxi_colour\"]\n",
    "numerical_cols = [\n",
    "    \"passenger_count\", \"trip_distance\", \"trip_time\", \"speed_mph\", \"extra\", \"mta_tax\",\n",
    "    \"tip_amount\", \"ehail_fee\", \"improvement_surcharge\", \"congestion_surcharge\", \"airport_fee\",\n",
    "    \"month\", \"day_of_week\", \"hour\"\n",
    "]\n",
    "target_col = \"total_amount\"\n",
    "all_cols_for_preprocessor = categorical_cols + numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e843864-6186-42c8-ab09-6e016159d0e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Sampling 200k rows to fit preprocessor...\n✅ Preprocessor fitted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Fit preprocessor on a small sample\n",
    "print(\"⏳ Sampling 200k rows to fit preprocessor...\")\n",
    "sample_df = df.limit(200_000).toPandas()\n",
    "\n",
    "numerical_transformer = sk_pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler(with_mean=False))  # with_mean=False for sparse matrices\n",
    "])\n",
    "categorical_transformer = sk_pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'  # drop any column not listed\n",
    ")\n",
    "\n",
    "preprocessor.fit(sample_df[all_cols_for_preprocessor])\n",
    "print(\"✅ Preprocessor fitted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6781e4d6-e430-4cc1-b69e-6c89db352a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 25689161\nTraining in 129 batches of up to 200000 rows.\n✅ Batch trained: 200000 rows, Time: 15.50s, Total rows processed: 200000\n✅ Batch trained: 200000 rows, Time: 15.71s, Total rows processed: 400000\n✅ Batch trained: 200000 rows, Time: 15.84s, Total rows processed: 600000\n✅ Batch trained: 200000 rows, Time: 15.93s, Total rows processed: 800000\n✅ Batch trained: 200000 rows, Time: 15.49s, Total rows processed: 1000000\n✅ Batch trained: 200000 rows, Time: 15.75s, Total rows processed: 1200000\n✅ Batch trained: 200000 rows, Time: 15.22s, Total rows processed: 1400000\n✅ Batch trained: 200000 rows, Time: 15.05s, Total rows processed: 1600000\n✅ Batch trained: 200000 rows, Time: 15.43s, Total rows processed: 1800000\n✅ Batch trained: 200000 rows, Time: 15.31s, Total rows processed: 2000000\n✅ Batch trained: 200000 rows, Time: 15.04s, Total rows processed: 2200000\n✅ Batch trained: 200000 rows, Time: 15.21s, Total rows processed: 2400000\n✅ Batch trained: 200000 rows, Time: 15.00s, Total rows processed: 2600000\n✅ Batch trained: 200000 rows, Time: 15.39s, Total rows processed: 2800000\n✅ Batch trained: 200000 rows, Time: 15.26s, Total rows processed: 3000000\n✅ Batch trained: 200000 rows, Time: 14.86s, Total rows processed: 3200000\n✅ Batch trained: 200000 rows, Time: 14.93s, Total rows processed: 3400000\n✅ Batch trained: 200000 rows, Time: 15.15s, Total rows processed: 3600000\n✅ Batch trained: 200000 rows, Time: 15.00s, Total rows processed: 3800000\n✅ Batch trained: 200000 rows, Time: 14.75s, Total rows processed: 4000000\n✅ Batch trained: 200000 rows, Time: 15.00s, Total rows processed: 4200000\n✅ Batch trained: 200000 rows, Time: 15.40s, Total rows processed: 4400000\n✅ Batch trained: 200000 rows, Time: 15.24s, Total rows processed: 4600000\n✅ Batch trained: 200000 rows, Time: 15.02s, Total rows processed: 4800000\n✅ Batch trained: 200000 rows, Time: 15.03s, Total rows processed: 5000000\n✅ Batch trained: 200000 rows, Time: 15.29s, Total rows processed: 5200000\n✅ Batch trained: 200000 rows, Time: 15.39s, Total rows processed: 5400000\n✅ Batch trained: 200000 rows, Time: 15.35s, Total rows processed: 5600000\n✅ Batch trained: 200000 rows, Time: 15.32s, Total rows processed: 5800000\n✅ Batch trained: 200000 rows, Time: 14.96s, Total rows processed: 6000000\n✅ Batch trained: 200000 rows, Time: 15.39s, Total rows processed: 6200000\n✅ Batch trained: 200000 rows, Time: 15.16s, Total rows processed: 6400000\n✅ Batch trained: 200000 rows, Time: 15.28s, Total rows processed: 6600000\n✅ Batch trained: 200000 rows, Time: 15.14s, Total rows processed: 6800000\n✅ Batch trained: 200000 rows, Time: 14.83s, Total rows processed: 7000000\n✅ Batch trained: 200000 rows, Time: 15.44s, Total rows processed: 7200000\n✅ Batch trained: 200000 rows, Time: 15.59s, Total rows processed: 7400000\n✅ Batch trained: 200000 rows, Time: 15.39s, Total rows processed: 7600000\n✅ Batch trained: 200000 rows, Time: 15.26s, Total rows processed: 7800000\n✅ Batch trained: 200000 rows, Time: 15.07s, Total rows processed: 8000000\n✅ Batch trained: 200000 rows, Time: 15.52s, Total rows processed: 8200000\n✅ Batch trained: 200000 rows, Time: 15.09s, Total rows processed: 8400000\n✅ Batch trained: 200000 rows, Time: 15.17s, Total rows processed: 8600000\n✅ Batch trained: 200000 rows, Time: 15.00s, Total rows processed: 8800000\n✅ Batch trained: 200000 rows, Time: 14.83s, Total rows processed: 9000000\n✅ Batch trained: 200000 rows, Time: 14.82s, Total rows processed: 9200000\n✅ Batch trained: 200000 rows, Time: 15.16s, Total rows processed: 9400000\n✅ Batch trained: 200000 rows, Time: 14.66s, Total rows processed: 9600000\n✅ Batch trained: 200000 rows, Time: 15.49s, Total rows processed: 9800000\n✅ Batch trained: 200000 rows, Time: 15.30s, Total rows processed: 10000000\n✅ Batch trained: 200000 rows, Time: 15.09s, Total rows processed: 10200000\n✅ Batch trained: 200000 rows, Time: 14.98s, Total rows processed: 10400000\n✅ Batch trained: 200000 rows, Time: 14.89s, Total rows processed: 10600000\n✅ Batch trained: 200000 rows, Time: 15.24s, Total rows processed: 10800000\n✅ Batch trained: 200000 rows, Time: 15.52s, Total rows processed: 11000000\n✅ Batch trained: 200000 rows, Time: 15.23s, Total rows processed: 11200000\n✅ Batch trained: 200000 rows, Time: 15.20s, Total rows processed: 11400000\n✅ Batch trained: 200000 rows, Time: 15.08s, Total rows processed: 11600000\n✅ Batch trained: 200000 rows, Time: 14.96s, Total rows processed: 11800000\n✅ Batch trained: 200000 rows, Time: 14.78s, Total rows processed: 12000000\n✅ Batch trained: 200000 rows, Time: 14.91s, Total rows processed: 12200000\n✅ Batch trained: 200000 rows, Time: 14.89s, Total rows processed: 12400000\n✅ Batch trained: 200000 rows, Time: 14.76s, Total rows processed: 12600000\n✅ Batch trained: 200000 rows, Time: 14.99s, Total rows processed: 12800000\n✅ Batch trained: 200000 rows, Time: 14.88s, Total rows processed: 13000000\n✅ Batch trained: 200000 rows, Time: 14.72s, Total rows processed: 13200000\n✅ Batch trained: 200000 rows, Time: 15.35s, Total rows processed: 13400000\n✅ Batch trained: 200000 rows, Time: 14.58s, Total rows processed: 13600000\n✅ Batch trained: 200000 rows, Time: 15.07s, Total rows processed: 13800000\n✅ Batch trained: 200000 rows, Time: 15.15s, Total rows processed: 14000000\n✅ Batch trained: 200000 rows, Time: 15.10s, Total rows processed: 14200000\n✅ Batch trained: 200000 rows, Time: 14.95s, Total rows processed: 14400000\n✅ Batch trained: 200000 rows, Time: 15.62s, Total rows processed: 14600000\n✅ Batch trained: 200000 rows, Time: 14.78s, Total rows processed: 14800000\n✅ Batch trained: 200000 rows, Time: 15.21s, Total rows processed: 15000000\n✅ Batch trained: 200000 rows, Time: 15.10s, Total rows processed: 15200000\n✅ Batch trained: 200000 rows, Time: 14.93s, Total rows processed: 15400000\n✅ Batch trained: 200000 rows, Time: 14.96s, Total rows processed: 15600000\n✅ Batch trained: 200000 rows, Time: 15.32s, Total rows processed: 15800000\n✅ Batch trained: 200000 rows, Time: 15.36s, Total rows processed: 16000000\n✅ Batch trained: 200000 rows, Time: 15.13s, Total rows processed: 16200000\n✅ Batch trained: 200000 rows, Time: 15.36s, Total rows processed: 16400000\n✅ Batch trained: 200000 rows, Time: 14.87s, Total rows processed: 16600000\n✅ Batch trained: 200000 rows, Time: 14.40s, Total rows processed: 16800000\n✅ Batch trained: 200000 rows, Time: 15.05s, Total rows processed: 17000000\n✅ Batch trained: 200000 rows, Time: 14.66s, Total rows processed: 17200000\n✅ Batch trained: 200000 rows, Time: 15.26s, Total rows processed: 17400000\n✅ Batch trained: 200000 rows, Time: 14.96s, Total rows processed: 17600000\n✅ Batch trained: 200000 rows, Time: 14.73s, Total rows processed: 17800000\n✅ Batch trained: 200000 rows, Time: 15.11s, Total rows processed: 18000000\n✅ Batch trained: 200000 rows, Time: 15.08s, Total rows processed: 18200000\n✅ Batch trained: 200000 rows, Time: 14.39s, Total rows processed: 18400000\n✅ Batch trained: 200000 rows, Time: 15.17s, Total rows processed: 18600000\n✅ Batch trained: 200000 rows, Time: 14.95s, Total rows processed: 18800000\n✅ Batch trained: 200000 rows, Time: 15.40s, Total rows processed: 19000000\n✅ Batch trained: 200000 rows, Time: 15.12s, Total rows processed: 19200000\n✅ Batch trained: 200000 rows, Time: 15.93s, Total rows processed: 19400000\n✅ Batch trained: 200000 rows, Time: 15.33s, Total rows processed: 19600000\n✅ Batch trained: 200000 rows, Time: 15.25s, Total rows processed: 19800000\n✅ Batch trained: 200000 rows, Time: 15.24s, Total rows processed: 20000000\n✅ Batch trained: 200000 rows, Time: 14.98s, Total rows processed: 20200000\n✅ Batch trained: 200000 rows, Time: 15.07s, Total rows processed: 20400000\n✅ Batch trained: 200000 rows, Time: 14.89s, Total rows processed: 20600000\n✅ Batch trained: 200000 rows, Time: 15.07s, Total rows processed: 20800000\n✅ Batch trained: 200000 rows, Time: 14.89s, Total rows processed: 21000000\n✅ Batch trained: 200000 rows, Time: 15.36s, Total rows processed: 21200000\n✅ Batch trained: 200000 rows, Time: 15.09s, Total rows processed: 21400000\n✅ Batch trained: 200000 rows, Time: 14.58s, Total rows processed: 21600000\n✅ Batch trained: 200000 rows, Time: 14.89s, Total rows processed: 21800000\n✅ Batch trained: 200000 rows, Time: 14.99s, Total rows processed: 22000000\n✅ Batch trained: 200000 rows, Time: 15.24s, Total rows processed: 22200000\n✅ Batch trained: 200000 rows, Time: 14.74s, Total rows processed: 22400000\n✅ Batch trained: 200000 rows, Time: 15.06s, Total rows processed: 22600000\n✅ Batch trained: 200000 rows, Time: 15.03s, Total rows processed: 22800000\n✅ Batch trained: 200000 rows, Time: 15.45s, Total rows processed: 23000000\n✅ Batch trained: 200000 rows, Time: 15.19s, Total rows processed: 23200000\n✅ Batch trained: 200000 rows, Time: 15.50s, Total rows processed: 23400000\n✅ Batch trained: 200000 rows, Time: 15.16s, Total rows processed: 23600000\n✅ Batch trained: 200000 rows, Time: 14.96s, Total rows processed: 23800000\n✅ Batch trained: 200000 rows, Time: 15.39s, Total rows processed: 24000000\n✅ Batch trained: 200000 rows, Time: 14.78s, Total rows processed: 24200000\n✅ Batch trained: 200000 rows, Time: 15.29s, Total rows processed: 24400000\n✅ Batch trained: 200000 rows, Time: 14.58s, Total rows processed: 24600000\n✅ Batch trained: 200000 rows, Time: 15.02s, Total rows processed: 24800000\n✅ Batch trained: 200000 rows, Time: 14.73s, Total rows processed: 25000000\n✅ Batch trained: 200000 rows, Time: 14.94s, Total rows processed: 25200000\n✅ Batch trained: 200000 rows, Time: 15.14s, Total rows processed: 25400000\n✅ Batch trained: 200000 rows, Time: 15.12s, Total rows processed: 25600000\n✅ Batch trained: 89161 rows, Time: 14.28s, Total rows processed: 25689161\n\uD83C\uDF89 Model trained on full dataset using partial_fit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize SGDRegressor for partial_fit\n",
    "model = SGDRegressor()\n",
    "\n",
    "BATCH_SIZE = 200_000\n",
    "offset = 0\n",
    "total_rows = df.count()\n",
    "num_batches = int(np.ceil(total_rows / BATCH_SIZE))\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Training in {num_batches} batches of up to {BATCH_SIZE} rows.\")\n",
    "\n",
    "\n",
    "# Batch-wise training\n",
    "while offset < total_rows:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use Spark SQL row_number for batching (avoid limit/offset issues)\n",
    "    batch_df = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER (ORDER BY total_amount) AS rn\n",
    "            FROM train_df_2024\n",
    "        ) tmp\n",
    "        WHERE rn > {offset} AND rn <= {offset + BATCH_SIZE}\n",
    "    \"\"\")\n",
    "\n",
    "    batch_pandas_df = batch_df.toPandas()\n",
    "    if batch_pandas_df.empty:\n",
    "        break\n",
    "\n",
    "    # Split features and target\n",
    "    X_batch = batch_pandas_df[all_cols_for_preprocessor]\n",
    "    y_batch = batch_pandas_df[target_col]\n",
    "\n",
    "    # Preprocess\n",
    "    X_batch_processed = preprocessor.transform(X_batch)\n",
    "\n",
    "    # Partial fit\n",
    "    model.partial_fit(X_batch_processed, y_batch)\n",
    "\n",
    "    offset += len(X_batch)\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ Batch trained: {len(X_batch)} rows, Time: {end_time - start_time:.2f}s, Total rows processed: {offset}\")\n",
    "\n",
    "print(\"\uD83C\uDF89 Model trained on full dataset using partial_fit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a172192a-d8e4-4b5b-b1d5-a11df15cfef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and preprocessor saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessor and model\n",
    "\n",
    "# Define the path\n",
    "save_path = \"/Volumes/workspace/bde/assignment2\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save them\n",
    "joblib.dump(model, os.path.join(save_path, \"sdgregressor_model.joblib\"))\n",
    "joblib.dump(preprocessor, os.path.join(save_path, \"preprocessor.joblib\"))\n",
    "\n",
    "print(\"✅ Model and preprocessor saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547ab779-613f-4285-9f31-ffb48902de02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the test_df to pandas dataframe\n",
    "test_df = test_df_2024.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4720884a-a9e4-4af9-a825-436275ed8676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create X_test and y_test\n",
    "X_test = test_df.drop(\"total_amount\",axis=1)\n",
    "y_test = test_df[\"total_amount\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ffb9d1-75b8-49b5-a251-84b55bf8f034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the X_test\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564a6db6-e0a4-48f8-87e8-470d4934117b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved X_test_processed and y_test as .npy files\n"
     ]
    }
   ],
   "source": [
    "# Due to long running time sometimes it shows there is no packages imported\n",
    "# So for safer side import numpy and os again\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "save_path = \"/Volumes/workspace/bde/assignment2\"\n",
    "\n",
    "np.save(os.path.join(save_path, \"X_test_processed.npy\"), X_test_processed)\n",
    "np.save(os.path.join(save_path, \"y_test.npy\"), y_test.to_numpy())\n",
    "\n",
    "print(\"✅ Saved X_test_processed and y_test as .npy files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4eeca2-2b6f-4c65-916a-ba1f1857e986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 73.64\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE for the test set\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "y_pred = model.predict(X_test_processed)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f\"RMSE: {rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "numpy",
     "scikit-learn",
     "threadpoolctl==3.2.0"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BDE_AT2_PART_3_Model_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}